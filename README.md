# Tokenization of Tibetan texts
This repository contains code for tokenizing a corpus Tibetan texts.

There are two Jupyter Notebooks that demonstrate word-based tokenization of Tibetan texts with Python.

One notebook uses [Esukhia's botok tokenizer](https://github.com/Esukhia/botok), the other trains a tokenizer based on the generic [sentencepiece tokenizer package](https://pypi.org/project/sentencepiece/) by running sentencepiece over a corpus of text to make it automatically decide on a suitable vocabulary.

