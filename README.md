# Tokenization of Tibetan texts
This repository contains code for tokenizing a corpus Tibetan texts.

There are two Jupyter Notebooks that demonstrate word-based tokenization of Tibetan texts with Python.

One notebook uses [https://github.com/Esukhia/botok](Esukhia's botok tokenizer), the other trains a tokenizer based on the generic [https://pypi.org/project/sentencepiece/](sentencepiece tokenizer package) by running sentencepiece over a corpus of text to make it automatically decide on a suitable vocabulary.

